{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "patient-tamil",
   "metadata": {},
   "source": [
    "# Using DebugResult\n",
    "\n",
    "Here, we will show how to use `DebugResult` to debug some problems we might encounter when using our mlir-opt CLI Wrapper.\n",
    "\n",
    "Letâ€™s first import some necessary classes and generate an instance of our mlir-opt CLI Wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparative-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlir_graphblas import MlirOptCli\n",
    "\n",
    "cli = MlirOptCli(executable=None, options=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-conviction",
   "metadata": {},
   "source": [
    "## Generate Example Input\n",
    "\n",
    "Let's say we have a bunch of MLIR code that we're not familiar with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intensive-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_string = \"\"\"\n",
    "#trait_sum_reduction = {\n",
    "  indexing_maps = [\n",
    "    affine_map<(i,j,k) -> (i,j,k)>,  // A\n",
    "    affine_map<(i,j,k) -> ()>        // x (scalar out)\n",
    "  ],\n",
    "  iterator_types = [\"reduction\", \"reduction\", \"reduction\"],\n",
    "  doc = \"x += SUM_ijk A(i,j,k)\"\n",
    "}\n",
    "\n",
    "#sparseTensor = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"compressed\", \"compressed\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j,k) -> (i,j,k)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "func @func_f32(%argA: tensor<10x20x30xf32, #sparseTensor>) -> f32 {\n",
    "  %out_tensor = constant dense<0.0> : tensor<f32>\n",
    "  %reduction = linalg.generic #trait_sum_reduction\n",
    "     ins(%argA: tensor<10x20x30xf32, #sparseTensor>)\n",
    "    outs(%out_tensor: tensor<f32>) {\n",
    "      ^bb(%a: f32, %x: f32):\n",
    "        %0 = addf %x, %a : f32\n",
    "        linalg.yield %0 : f32\n",
    "  } -> tensor<f32>\n",
    "  %answer = tensor.extract %reduction[] : tensor<f32>\n",
    "  return %answer : f32\n",
    "}\n",
    "\"\"\"\n",
    "mlir_bytes = mlir_string.encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-accounting",
   "metadata": {},
   "source": [
    "Since we're not familiar with this code, we don't exactly know what passes are necessary or in what order they should go in.\n",
    "\n",
    "Let's say that this is the first set of passes we try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aggregate-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = [\n",
    "    \"--sparsification\",\n",
    "    \"--sparse-tensor-conversion\",\n",
    "    \"--linalg-bufferize\",\n",
    "    \"--func-bufferize\",\n",
    "    \"--tensor-bufferize\",\n",
    "    \"--tensor-constant-bufferize\",\n",
    "    \"--finalizing-bufferize\",\n",
    "    \"--convert-linalg-to-loops\",\n",
    "    \"--convert-memref-to-llvm\",\n",
    "    \"--convert-std-to-llvm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-excitement",
   "metadata": {},
   "source": [
    "Let's see what results we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fifty-smooth",
   "metadata": {},
   "outputs": [
    {
     "ename": "MlirOptError",
     "evalue": "<stdin>:20:16: error: failed to legalize operation 'builtin.unrealized_conversion_cast' that was explicitly marked illegal\n  %reduction = linalg.generic #trait_sum_reduction\n               ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlirOptError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yj/nmf5xtns3hx6qgdnybj964q80000gp/T/ipykernel_86998/2890307325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_passes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlir_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/mlir-graphblas/mlir_graphblas/cli.py\u001b[0m in \u001b[0;36mapply_passes\u001b[0;34m(self, file, passes)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMlirOptError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merr_lines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_passes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpasses\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_passes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DebugResult\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMlirOptError\u001b[0m: <stdin>:20:16: error: failed to legalize operation 'builtin.unrealized_conversion_cast' that was explicitly marked illegal\n  %reduction = linalg.generic #trait_sum_reduction\n               ^"
     ]
    }
   ],
   "source": [
    "result = cli.apply_passes(mlir_bytes, passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-japanese",
   "metadata": {},
   "source": [
    "We get an exception. \n",
    "\n",
    "Unfortunately, the exception message isn't very clear as it only gives us the immediate error message but doesn't inform us of the context in which it occurred, e.g. in which pass the error occurred (if any) or if any necessary passes are missing. \n",
    "\n",
    "We only know that the operation `builtin.unrealized_conversion_cast` shows up somewhere and that it's a problem.\n",
    "\n",
    "Let's try to use the `debug_passes` method instead of the `apply_passes` to get more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "broke-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cli.debug_passes(mlir_bytes, passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affecting-addiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================\n",
       "  Error when running convert-std-to-llvm  \n",
       "==========================================\n",
       "<stdin>:77:13: error: failed to legalize operation 'builtin.unrealized_conversion_cast' that was explicitly marked illegal\n",
       "      %62 = builtin.unrealized_conversion_cast %arg1 : index to i64\n",
       "            ^\n",
       "<stdin>:77:13: note: see current operation: %72 = \"builtin.unrealized_conversion_cast\"(%arg1) : (index) -> i64 loc(\"<stdin>\":77:13)\n",
       "\n",
       "\n",
       "================================\n",
       "  Input to convert-std-to-llvm  \n",
       "================================\n",
       "             10        20        30        40        50        60        70        80        90        100       110       120       130       140       \n",
       "    1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345\n",
       "    -------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "  1|builtin.module  {\n",
       "  2|  llvm.func @memrefCopy(i64, !llvm.ptr<struct<(i64, ptr<i8>)>>, !llvm.ptr<struct<(i64, ptr<i8>)>>)\n",
       "  3|  llvm.func @malloc(i64) -> !llvm.ptr<i8>\n",
       "  4|  llvm.mlir.global private constant @__constant_xf32(0.000000e+00 : f32) : f32\n",
       "  5|  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
       "  6|  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "  7|  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
       "  8|    %c2 = constant 2 : index\n",
       "  9|    %c0 = constant 0 : index\n",
       " 10|    %c1 = constant 1 : index\n",
       " 11|    %0 = llvm.mlir.constant(1 : index) : i64\n",
       " 12|    %1 = llvm.mlir.null : !llvm.ptr<f32>\n",
       " 13|    %2 = llvm.getelementptr %1[%0] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>\n",
       " 14|    %3 = llvm.ptrtoint %2 : !llvm.ptr<f32> to i64\n",
       " 15|    %4 = llvm.mlir.addressof @__constant_xf32 : !llvm.ptr<f32>\n",
       " 16|    %5 = llvm.mlir.constant(0 : index) : i64\n",
       " 17|    %6 = llvm.getelementptr %4[%5] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>\n",
       " 18|    %7 = llvm.mlir.constant(3735928559 : index) : i64\n",
       " 19|    %8 = llvm.inttoptr %7 : i64 to !llvm.ptr<f32>\n",
       " 20|    %9 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       " 21|    %10 = llvm.insertvalue %8, %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       " 22|    %11 = llvm.insertvalue %6, %10[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       " 23|    %12 = llvm.mlir.constant(0 : index) : i64\n",
       " 24|    %13 = llvm.insertvalue %12, %11[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       " 25|    %14 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       " 26|    %15 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       " 27|    %16 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       " 28|    %17 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
       " 29|    %18 = llvm.mlir.constant(1 : index) : i64\n",
       " 30|    %19 = llvm.mlir.null : !llvm.ptr<f32>\n",
       " 31|    %20 = llvm.getelementptr %19[%18] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>\n",
       " 32|    %21 = llvm.ptrtoint %20 : !llvm.ptr<f32> to i64\n",
       " 33|    %22 = llvm.call @malloc(%21) : (i64) -> !llvm.ptr<i8>\n",
       " 34|    %23 = llvm.bitcast %22 : !llvm.ptr<i8> to !llvm.ptr<f32>\n",
       " 35|    %24 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       " 36|    %25 = llvm.insertvalue %23, %24[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       " 37|    %26 = llvm.insertvalue %23, %25[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       " 38|    %27 = llvm.mlir.constant(0 : index) : i64\n",
       " 39|    %28 = llvm.insertvalue %27, %26[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       " 40|    %29 = llvm.mlir.constant(0 : index) : i64\n",
       " 41|    %30 = llvm.mlir.constant(1 : index) : i64\n",
       " 42|    %31 = llvm.alloca %30 x !llvm.struct<(ptr<f32>, ptr<f32>, i64)> : (i64) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64)>>\n",
       " 43|    llvm.store %13, %31 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64)>>\n",
       " 44|    %32 = llvm.bitcast %31 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64)>> to !llvm.ptr<i8>\n",
       " 45|    %33 = llvm.mlir.undef : !llvm.struct<(i64, ptr<i8>)>\n",
       " 46|    %34 = llvm.insertvalue %29, %33[0] : !llvm.struct<(i64, ptr<i8>)>\n",
       " 47|    %35 = llvm.insertvalue %32, %34[1] : !llvm.struct<(i64, ptr<i8>)>\n",
       " 48|    %36 = llvm.mlir.constant(0 : index) : i64\n",
       " 49|    %37 = llvm.mlir.constant(1 : index) : i64\n",
       " 50|    %38 = llvm.alloca %37 x !llvm.struct<(ptr<f32>, ptr<f32>, i64)> : (i64) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64)>>\n",
       " 51|    llvm.store %28, %38 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64)>>\n",
       " 52|    %39 = llvm.bitcast %38 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64)>> to !llvm.ptr<i8>\n",
       " 53|    %40 = llvm.mlir.undef : !llvm.struct<(i64, ptr<i8>)>\n",
       " 54|    %41 = llvm.insertvalue %36, %40[0] : !llvm.struct<(i64, ptr<i8>)>\n",
       " 55|    %42 = llvm.insertvalue %39, %41[1] : !llvm.struct<(i64, ptr<i8>)>\n",
       " 56|    %43 = llvm.mlir.constant(1 : index) : i64\n",
       " 57|    %44 = llvm.alloca %43 x !llvm.struct<(i64, ptr<i8>)> : (i64) -> !llvm.ptr<struct<(i64, ptr<i8>)>>\n",
       " 58|    llvm.store %35, %44 : !llvm.ptr<struct<(i64, ptr<i8>)>>\n",
       " 59|    %45 = llvm.alloca %43 x !llvm.struct<(i64, ptr<i8>)> : (i64) -> !llvm.ptr<struct<(i64, ptr<i8>)>>\n",
       " 60|    llvm.store %42, %45 : !llvm.ptr<struct<(i64, ptr<i8>)>>\n",
       " 61|    %46 = llvm.mlir.constant(4 : index) : i64\n",
       " 62|    llvm.call @memrefCopy(%46, %44, %45) : (i64, !llvm.ptr<struct<(i64, ptr<i8>)>>, !llvm.ptr<struct<(i64, ptr<i8>)>>) -> ()\n",
       " 63|    %47 = builtin.unrealized_conversion_cast %14 : memref<?xi64> to !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 64|    %48 = builtin.unrealized_conversion_cast %c0 : index to i64\n",
       " 65|    %49 = llvm.extractvalue %47[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 66|    %50 = llvm.getelementptr %49[%48] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>\n",
       " 67|    %51 = llvm.load %50 : !llvm.ptr<i64>\n",
       " 68|    %52 = index_cast %51 : i64 to index\n",
       " 69|    %53 = builtin.unrealized_conversion_cast %14 : memref<?xi64> to !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 70|    %54 = builtin.unrealized_conversion_cast %c1 : index to i64\n",
       " 71|    %55 = llvm.extractvalue %53[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 72|    %56 = llvm.getelementptr %55[%54] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>\n",
       " 73|    %57 = llvm.load %56 : !llvm.ptr<i64>\n",
       " 74|    %58 = index_cast %57 : i64 to index\n",
       " 75|    scf.for %arg1 = %52 to %58 step %c1 {\n",
       " 76|      %61 = builtin.unrealized_conversion_cast %15 : memref<?xi64> to !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 77|      %62 = builtin.unrealized_conversion_cast %arg1 : index to i64\n",
       " 78|      %63 = llvm.extractvalue %61[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 79|      %64 = llvm.getelementptr %63[%62] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>\n",
       " 80|      %65 = llvm.load %64 : !llvm.ptr<i64>\n",
       " 81|      %66 = index_cast %65 : i64 to index\n",
       " 82|      %67 = addi %arg1, %c1 : index\n",
       " 83|      %68 = builtin.unrealized_conversion_cast %15 : memref<?xi64> to !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 84|      %69 = builtin.unrealized_conversion_cast %67 : index to i64\n",
       " 85|      %70 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 86|      %71 = llvm.getelementptr %70[%69] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>\n",
       " 87|      %72 = llvm.load %71 : !llvm.ptr<i64>\n",
       " 88|      %73 = index_cast %72 : i64 to index\n",
       " 89|      scf.for %arg2 = %66 to %73 step %c1 {\n",
       " 90|        %74 = builtin.unrealized_conversion_cast %16 : memref<?xi64> to !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 91|        %75 = builtin.unrealized_conversion_cast %arg2 : index to i64\n",
       " 92|        %76 = llvm.extractvalue %74[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 93|        %77 = llvm.getelementptr %76[%75] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>\n",
       " 94|        %78 = llvm.load %77 : !llvm.ptr<i64>\n",
       " 95|        %79 = index_cast %78 : i64 to index\n",
       " 96|        %80 = addi %arg2, %c1 : index\n",
       " 97|        %81 = builtin.unrealized_conversion_cast %16 : memref<?xi64> to !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       " 98|        %82 = builtin.unrealized_conversion_cast %80 : index to i64\n",
       " 99|        %83 = llvm.extractvalue %81[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>\n",
       "100|        %84 = llvm.getelementptr %83[%82] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>\n",
       "101|        %85 = llvm.load %84 : !llvm.ptr<i64>\n",
       "102|        %86 = index_cast %85 : i64 to index\n",
       "103|        %87 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       "104|        %88 = llvm.load %87 : !llvm.ptr<f32>\n",
       "105|        %89 = scf.for %arg3 = %79 to %86 step %c1 iter_args(%arg4 = %88) -> (f32) {\n",
       "106|          %91 = builtin.unrealized_conversion_cast %17 : memref<?xf32> to !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>\n",
       "107|          %92 = builtin.unrealized_conversion_cast %arg3 : index to i64\n",
       "108|          %93 = llvm.extractvalue %91[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>\n",
       "109|          %94 = llvm.getelementptr %93[%92] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>\n",
       "110|          %95 = llvm.load %94 : !llvm.ptr<f32>\n",
       "111|          %96 = addf %arg4, %95 : f32\n",
       "112|          scf.yield %96 : f32\n",
       "113|        }\n",
       "114|        %90 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       "115|        llvm.store %89, %90 : !llvm.ptr<f32>\n",
       "116|      }\n",
       "117|    }\n",
       "118|    %59 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
       "119|    %60 = llvm.load %59 : !llvm.ptr<f32>\n",
       "120|    return %60 : f32\n",
       "121|  }\n",
       "122|}\n",
       "123|\n",
       "\n",
       "===================================\n",
       "  Input to convert-memref-to-llvm  \n",
       "===================================\n",
       "builtin.module  {\n",
       "  memref.global \"private\" constant @__constant_xf32 : memref<f32> = dense<0.000000e+00>\n",
       "  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
       "  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
       "    %c2 = constant 2 : index\n",
       "    %c0 = constant 0 : index\n",
       "    %c1 = constant 1 : index\n",
       "    %0 = memref.get_global @__constant_xf32 : memref<f32>\n",
       "    %1 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %2 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %3 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %4 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
       "    %5 = memref.alloc() : memref<f32>\n",
       "    memref.copy %0, %5 : memref<f32> to memref<f32>\n",
       "    %6 = memref.load %1[%c0] : memref<?xi64>\n",
       "    %7 = index_cast %6 : i64 to index\n",
       "    %8 = memref.load %1[%c1] : memref<?xi64>\n",
       "    %9 = index_cast %8 : i64 to index\n",
       "    scf.for %arg1 = %7 to %9 step %c1 {\n",
       "      %11 = memref.load %2[%arg1] : memref<?xi64>\n",
       "      %12 = index_cast %11 : i64 to index\n",
       "      %13 = addi %arg1, %c1 : index\n",
       "      %14 = memref.load %2[%13] : memref<?xi64>\n",
       "      %15 = index_cast %14 : i64 to index\n",
       "      scf.for %arg2 = %12 to %15 step %c1 {\n",
       "        %16 = memref.load %3[%arg2] : memref<?xi64>\n",
       "        %17 = index_cast %16 : i64 to index\n",
       "        %18 = addi %arg2, %c1 : index\n",
       "        %19 = memref.load %3[%18] : memref<?xi64>\n",
       "        %20 = index_cast %19 : i64 to index\n",
       "        %21 = memref.load %5[] : memref<f32>\n",
       "        %22 = scf.for %arg3 = %17 to %20 step %c1 iter_args(%arg4 = %21) -> (f32) {\n",
       "          %23 = memref.load %4[%arg3] : memref<?xf32>\n",
       "          %24 = addf %arg4, %23 : f32\n",
       "          scf.yield %24 : f32\n",
       "        }\n",
       "        memref.store %22, %5[] : memref<f32>\n",
       "      }\n",
       "    }\n",
       "    %10 = memref.load %5[] : memref<f32>\n",
       "    return %10 : f32\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "====================================\n",
       "  Input to convert-linalg-to-loops  \n",
       "====================================\n",
       "builtin.module  {\n",
       "  memref.global \"private\" constant @__constant_xf32 : memref<f32> = dense<0.000000e+00>\n",
       "  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
       "  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
       "    %0 = memref.get_global @__constant_xf32 : memref<f32>\n",
       "    %c2 = constant 2 : index\n",
       "    %c0 = constant 0 : index\n",
       "    %c1 = constant 1 : index\n",
       "    %1 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %2 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %3 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %4 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
       "    %5 = memref.alloc() : memref<f32>\n",
       "    memref.copy %0, %5 : memref<f32> to memref<f32>\n",
       "    %6 = memref.load %1[%c0] : memref<?xi64>\n",
       "    %7 = index_cast %6 : i64 to index\n",
       "    %8 = memref.load %1[%c1] : memref<?xi64>\n",
       "    %9 = index_cast %8 : i64 to index\n",
       "    scf.for %arg1 = %7 to %9 step %c1 {\n",
       "      %11 = memref.load %2[%arg1] : memref<?xi64>\n",
       "      %12 = index_cast %11 : i64 to index\n",
       "      %13 = addi %arg1, %c1 : index\n",
       "      %14 = memref.load %2[%13] : memref<?xi64>\n",
       "      %15 = index_cast %14 : i64 to index\n",
       "      scf.for %arg2 = %12 to %15 step %c1 {\n",
       "        %16 = memref.load %3[%arg2] : memref<?xi64>\n",
       "        %17 = index_cast %16 : i64 to index\n",
       "        %18 = addi %arg2, %c1 : index\n",
       "        %19 = memref.load %3[%18] : memref<?xi64>\n",
       "        %20 = index_cast %19 : i64 to index\n",
       "        %21 = memref.load %5[] : memref<f32>\n",
       "        %22 = scf.for %arg3 = %17 to %20 step %c1 iter_args(%arg4 = %21) -> (f32) {\n",
       "          %23 = memref.load %4[%arg3] : memref<?xf32>\n",
       "          %24 = addf %arg4, %23 : f32\n",
       "          scf.yield %24 : f32\n",
       "        }\n",
       "        memref.store %22, %5[] : memref<f32>\n",
       "      }\n",
       "    }\n",
       "    %10 = memref.load %5[] : memref<f32>\n",
       "    return %10 : f32\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "=================================\n",
       "  Input to finalizing-bufferize  \n",
       "=================================\n",
       "builtin.module  {\n",
       "  memref.global \"private\" constant @__constant_xf32 : memref<f32> = dense<0.000000e+00>\n",
       "  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
       "  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
       "    %0 = memref.get_global @__constant_xf32 : memref<f32>\n",
       "    %1 = memref.tensor_load %0 : memref<f32>\n",
       "    %c2 = constant 2 : index\n",
       "    %c0 = constant 0 : index\n",
       "    %c1 = constant 1 : index\n",
       "    %2 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %3 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %4 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %5 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
       "    %6 = memref.buffer_cast %1 : memref<f32>\n",
       "    %7 = memref.alloc() : memref<f32>\n",
       "    memref.copy %6, %7 : memref<f32> to memref<f32>\n",
       "    %8 = memref.load %2[%c0] : memref<?xi64>\n",
       "    %9 = index_cast %8 : i64 to index\n",
       "    %10 = memref.load %2[%c1] : memref<?xi64>\n",
       "    %11 = index_cast %10 : i64 to index\n",
       "    scf.for %arg1 = %9 to %11 step %c1 {\n",
       "      %15 = memref.load %3[%arg1] : memref<?xi64>\n",
       "      %16 = index_cast %15 : i64 to index\n",
       "      %17 = addi %arg1, %c1 : index\n",
       "      %18 = memref.load %3[%17] : memref<?xi64>\n",
       "      %19 = index_cast %18 : i64 to index\n",
       "      scf.for %arg2 = %16 to %19 step %c1 {\n",
       "        %20 = memref.load %4[%arg2] : memref<?xi64>\n",
       "        %21 = index_cast %20 : i64 to index\n",
       "        %22 = addi %arg2, %c1 : index\n",
       "        %23 = memref.load %4[%22] : memref<?xi64>\n",
       "        %24 = index_cast %23 : i64 to index\n",
       "        %25 = memref.load %7[] : memref<f32>\n",
       "        %26 = scf.for %arg3 = %21 to %24 step %c1 iter_args(%arg4 = %25) -> (f32) {\n",
       "          %27 = memref.load %5[%arg3] : memref<?xf32>\n",
       "          %28 = addf %arg4, %27 : f32\n",
       "          scf.yield %28 : f32\n",
       "        }\n",
       "        memref.store %26, %7[] : memref<f32>\n",
       "      }\n",
       "    }\n",
       "    %12 = memref.tensor_load %7 : memref<f32>\n",
       "    %13 = memref.buffer_cast %12 : memref<f32>\n",
       "    %14 = memref.load %13[] : memref<f32>\n",
       "    return %14 : f32\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "======================================\n",
       "  Input to tensor-constant-bufferize  \n",
       "======================================\n",
       "builtin.module  {\n",
       "  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
       "  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
       "    %cst = constant dense<0.000000e+00> : tensor<f32>\n",
       "    %c2 = constant 2 : index\n",
       "    %c0 = constant 0 : index\n",
       "    %c1 = constant 1 : index\n",
       "    %0 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %1 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %2 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %3 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
       "    %4 = memref.buffer_cast %cst : memref<f32>\n",
       "    %5 = memref.alloc() : memref<f32>\n",
       "    memref.copy %4, %5 : memref<f32> to memref<f32>\n",
       "    %6 = memref.load %0[%c0] : memref<?xi64>\n",
       "    %7 = index_cast %6 : i64 to index\n",
       "    %8 = memref.load %0[%c1] : memref<?xi64>\n",
       "    %9 = index_cast %8 : i64 to index\n",
       "    scf.for %arg1 = %7 to %9 step %c1 {\n",
       "      %13 = memref.load %1[%arg1] : memref<?xi64>\n",
       "      %14 = index_cast %13 : i64 to index\n",
       "      %15 = addi %arg1, %c1 : index\n",
       "      %16 = memref.load %1[%15] : memref<?xi64>\n",
       "      %17 = index_cast %16 : i64 to index\n",
       "      scf.for %arg2 = %14 to %17 step %c1 {\n",
       "        %18 = memref.load %2[%arg2] : memref<?xi64>\n",
       "        %19 = index_cast %18 : i64 to index\n",
       "        %20 = addi %arg2, %c1 : index\n",
       "        %21 = memref.load %2[%20] : memref<?xi64>\n",
       "        %22 = index_cast %21 : i64 to index\n",
       "        %23 = memref.load %5[] : memref<f32>\n",
       "        %24 = scf.for %arg3 = %19 to %22 step %c1 iter_args(%arg4 = %23) -> (f32) {\n",
       "          %25 = memref.load %3[%arg3] : memref<?xf32>\n",
       "          %26 = addf %arg4, %25 : f32\n",
       "          scf.yield %26 : f32\n",
       "        }\n",
       "        memref.store %24, %5[] : memref<f32>\n",
       "      }\n",
       "    }\n",
       "    %10 = memref.tensor_load %5 : memref<f32>\n",
       "    %11 = memref.buffer_cast %10 : memref<f32>\n",
       "    %12 = memref.load %11[] : memref<f32>\n",
       "    return %12 : f32\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "=============================\n",
       "  Input to tensor-bufferize  \n",
       "=============================\n",
       "builtin.module  {\n",
       "  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
       "  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
       "    %cst = constant dense<0.000000e+00> : tensor<f32>\n",
       "    %c2 = constant 2 : index\n",
       "    %c0 = constant 0 : index\n",
       "    %c1 = constant 1 : index\n",
       "    %0 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %1 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %2 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %3 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
       "    %4 = memref.buffer_cast %cst : memref<f32>\n",
       "    %5 = memref.alloc() : memref<f32>\n",
       "    memref.copy %4, %5 : memref<f32> to memref<f32>\n",
       "    %6 = memref.load %0[%c0] : memref<?xi64>\n",
       "    %7 = index_cast %6 : i64 to index\n",
       "    %8 = memref.load %0[%c1] : memref<?xi64>\n",
       "    %9 = index_cast %8 : i64 to index\n",
       "    scf.for %arg1 = %7 to %9 step %c1 {\n",
       "      %12 = memref.load %1[%arg1] : memref<?xi64>\n",
       "      %13 = index_cast %12 : i64 to index\n",
       "      %14 = addi %arg1, %c1 : index\n",
       "      %15 = memref.load %1[%14] : memref<?xi64>\n",
       "      %16 = index_cast %15 : i64 to index\n",
       "      scf.for %arg2 = %13 to %16 step %c1 {\n",
       "        %17 = memref.load %2[%arg2] : memref<?xi64>\n",
       "        %18 = index_cast %17 : i64 to index\n",
       "        %19 = addi %arg2, %c1 : index\n",
       "        %20 = memref.load %2[%19] : memref<?xi64>\n",
       "        %21 = index_cast %20 : i64 to index\n",
       "        %22 = memref.load %5[] : memref<f32>\n",
       "        %23 = scf.for %arg3 = %18 to %21 step %c1 iter_args(%arg4 = %22) -> (f32) {\n",
       "          %24 = memref.load %3[%arg3] : memref<?xf32>\n",
       "          %25 = addf %arg4, %24 : f32\n",
       "          scf.yield %25 : f32\n",
       "        }\n",
       "        memref.store %23, %5[] : memref<f32>\n",
       "      }\n",
       "    }\n",
       "    %10 = memref.tensor_load %5 : memref<f32>\n",
       "    %11 = tensor.extract %10[] : tensor<f32>\n",
       "    return %11 : f32\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "===========================\n",
       "  Input to func-bufferize  \n",
       "===========================\n",
       "builtin.module  {\n",
       "  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
       "  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
       "    %cst = constant dense<0.000000e+00> : tensor<f32>\n",
       "    %c2 = constant 2 : index\n",
       "    %c0 = constant 0 : index\n",
       "    %c1 = constant 1 : index\n",
       "    %0 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %1 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %2 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %3 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
       "    %4 = memref.buffer_cast %cst : memref<f32>\n",
       "    %5 = memref.alloc() : memref<f32>\n",
       "    memref.copy %4, %5 : memref<f32> to memref<f32>\n",
       "    %6 = memref.load %0[%c0] : memref<?xi64>\n",
       "    %7 = index_cast %6 : i64 to index\n",
       "    %8 = memref.load %0[%c1] : memref<?xi64>\n",
       "    %9 = index_cast %8 : i64 to index\n",
       "    scf.for %arg1 = %7 to %9 step %c1 {\n",
       "      %12 = memref.load %1[%arg1] : memref<?xi64>\n",
       "      %13 = index_cast %12 : i64 to index\n",
       "      %14 = addi %arg1, %c1 : index\n",
       "      %15 = memref.load %1[%14] : memref<?xi64>\n",
       "      %16 = index_cast %15 : i64 to index\n",
       "      scf.for %arg2 = %13 to %16 step %c1 {\n",
       "        %17 = memref.load %2[%arg2] : memref<?xi64>\n",
       "        %18 = index_cast %17 : i64 to index\n",
       "        %19 = addi %arg2, %c1 : index\n",
       "        %20 = memref.load %2[%19] : memref<?xi64>\n",
       "        %21 = index_cast %20 : i64 to index\n",
       "        %22 = memref.load %5[] : memref<f32>\n",
       "        %23 = scf.for %arg3 = %18 to %21 step %c1 iter_args(%arg4 = %22) -> (f32) {\n",
       "          %24 = memref.load %3[%arg3] : memref<?xf32>\n",
       "          %25 = addf %arg4, %24 : f32\n",
       "          scf.yield %25 : f32\n",
       "        }\n",
       "        memref.store %23, %5[] : memref<f32>\n",
       "      }\n",
       "    }\n",
       "    %10 = memref.tensor_load %5 : memref<f32>\n",
       "    %11 = tensor.extract %10[] : tensor<f32>\n",
       "    return %11 : f32\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "=============================\n",
       "  Input to linalg-bufferize  \n",
       "=============================\n",
       "builtin.module  {\n",
       "  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
       "  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
       "    %cst = constant dense<0.000000e+00> : tensor<f32>\n",
       "    %c2 = constant 2 : index\n",
       "    %c0 = constant 0 : index\n",
       "    %c1 = constant 1 : index\n",
       "    %0 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %1 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %2 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
       "    %3 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
       "    %4 = memref.buffer_cast %cst : memref<f32>\n",
       "    %5 = memref.alloc() : memref<f32>\n",
       "    memref.copy %4, %5 : memref<f32> to memref<f32>\n",
       "    %6 = memref.load %0[%c0] : memref<?xi64>\n",
       "    %7 = index_cast %6 : i64 to index\n",
       "    %8 = memref.load %0[%c1] : memref<?xi64>\n",
       "    %9 = index_cast %8 : i64 to index\n",
       "    scf.for %arg1 = %7 to %9 step %c1 {\n",
       "      %12 = memref.load %1[%arg1] : memref<?xi64>\n",
       "      %13 = index_cast %12 : i64 to index\n",
       "      %14 = addi %arg1, %c1 : index\n",
       "      %15 = memref.load %1[%14] : memref<?xi64>\n",
       "      %16 = index_cast %15 : i64 to index\n",
       "      scf.for %arg2 = %13 to %16 step %c1 {\n",
       "        %17 = memref.load %2[%arg2] : memref<?xi64>\n",
       "        %18 = index_cast %17 : i64 to index\n",
       "        %19 = addi %arg2, %c1 : index\n",
       "        %20 = memref.load %2[%19] : memref<?xi64>\n",
       "        %21 = index_cast %20 : i64 to index\n",
       "        %22 = memref.load %5[] : memref<f32>\n",
       "        %23 = scf.for %arg3 = %18 to %21 step %c1 iter_args(%arg4 = %22) -> (f32) {\n",
       "          %24 = memref.load %3[%arg3] : memref<?xf32>\n",
       "          %25 = addf %arg4, %24 : f32\n",
       "          scf.yield %25 : f32\n",
       "        }\n",
       "        memref.store %23, %5[] : memref<f32>\n",
       "      }\n",
       "    }\n",
       "    %10 = memref.tensor_load %5 : memref<f32>\n",
       "    %11 = tensor.extract %10[] : tensor<f32>\n",
       "    return %11 : f32\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "=====================================\n",
       "  Input to sparse-tensor-conversion  \n",
       "=====================================\n",
       "builtin.module  {\n",
       "  builtin.func @func_f32(%arg0: tensor<10x20x30xf32, #sparse_tensor.encoding<{ dimLevelType = [ \"compressed\", \"compressed\", \"compressed\" ], dimOrdering = affine_map<(d0, d1, d2) -> (d0, d1, d2)>, pointerBitWidth = 64, indexBitWidth = 64 }>>) -> f32 {\n",
       "    %cst = constant dense<0.000000e+00> : tensor<f32>\n",
       "    %c2 = constant 2 : index\n",
       "    %c0 = constant 0 : index\n",
       "    %c1 = constant 1 : index\n",
       "    %0 = sparse_tensor.pointers %arg0, %c0 : tensor<10x20x30xf32, #sparse_tensor.encoding<{ dimLevelType = [ \"compressed\", \"compressed\", \"compressed\" ], dimOrdering = affine_map<(d0, d1, d2) -> (d0, d1, d2)>, pointerBitWidth = 64, indexBitWidth = 64 }>> to memref<?xi64>\n",
       "    %1 = sparse_tensor.pointers %arg0, %c1 : tensor<10x20x30xf32, #sparse_tensor.encoding<{ dimLevelType = [ \"compressed\", \"compressed\", \"compressed\" ], dimOrdering = affine_map<(d0, d1, d2) -> (d0, d1, d2)>, pointerBitWidth = 64, indexBitWidth = 64 }>> to memref<?xi64>\n",
       "    %2 = sparse_tensor.pointers %arg0, %c2 : tensor<10x20x30xf32, #sparse_tensor.encoding<{ dimLevelType = [ \"compressed\", \"compressed\", \"compressed\" ], dimOrdering = affine_map<(d0, d1, d2) -> (d0, d1, d2)>, pointerBitWidth = 64, indexBitWidth = 64 }>> to memref<?xi64>\n",
       "    %3 = sparse_tensor.values %arg0 : tensor<10x20x30xf32, #sparse_tensor.encoding<{ dimLevelType = [ \"compressed\", \"compressed\", \"compressed\" ], dimOrdering = affine_map<(d0, d1, d2) -> (d0, d1, d2)>, pointerBitWidth = 64, indexBitWidth = 64 }>> to memref<?xf32>\n",
       "    %4 = memref.buffer_cast %cst : memref<f32>\n",
       "    %5 = memref.alloc() : memref<f32>\n",
       "    memref.copy %4, %5 : memref<f32> to memref<f32>\n",
       "    %6 = memref.load %0[%c0] : memref<?xi64>\n",
       "    %7 = index_cast %6 : i64 to index\n",
       "    %8 = memref.load %0[%c1] : memref<?xi64>\n",
       "    %9 = index_cast %8 : i64 to index\n",
       "    scf.for %arg1 = %7 to %9 step %c1 {\n",
       "      %12 = memref.load %1[%arg1] : memref<?xi64>\n",
       "      %13 = index_cast %12 : i64 to index\n",
       "      %14 = addi %arg1, %c1 : index\n",
       "      %15 = memref.load %1[%14] : memref<?xi64>\n",
       "      %16 = index_cast %15 : i64 to index\n",
       "      scf.for %arg2 = %13 to %16 step %c1 {\n",
       "        %17 = memref.load %2[%arg2] : memref<?xi64>\n",
       "        %18 = index_cast %17 : i64 to index\n",
       "        %19 = addi %arg2, %c1 : index\n",
       "        %20 = memref.load %2[%19] : memref<?xi64>\n",
       "        %21 = index_cast %20 : i64 to index\n",
       "        %22 = memref.load %5[] : memref<f32>\n",
       "        %23 = scf.for %arg3 = %18 to %21 step %c1 iter_args(%arg4 = %22) -> (f32) {\n",
       "          %24 = memref.load %3[%arg3] : memref<?xf32>\n",
       "          %25 = addf %arg4, %24 : f32\n",
       "          scf.yield %25 : f32\n",
       "        }\n",
       "        memref.store %23, %5[] : memref<f32>\n",
       "      }\n",
       "    }\n",
       "    %10 = memref.tensor_load %5 : memref<f32>\n",
       "    %11 = tensor.extract %10[] : tensor<f32>\n",
       "    return %11 : f32\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "===========================\n",
       "  Input to sparsification  \n",
       "===========================\n",
       "\n",
       "#trait_sum_reduction = {\n",
       "  indexing_maps = [\n",
       "    affine_map<(i,j,k) -> (i,j,k)>,  // A\n",
       "    affine_map<(i,j,k) -> ()>        // x (scalar out)\n",
       "  ],\n",
       "  iterator_types = [\"reduction\", \"reduction\", \"reduction\"],\n",
       "  doc = \"x += SUM_ijk A(i,j,k)\"\n",
       "}\n",
       "\n",
       "#sparseTensor = #sparse_tensor.encoding<{\n",
       "  dimLevelType = [ \"compressed\", \"compressed\", \"compressed\" ],\n",
       "  dimOrdering = affine_map<(i,j,k) -> (i,j,k)>,\n",
       "  pointerBitWidth = 64,\n",
       "  indexBitWidth = 64\n",
       "}>\n",
       "\n",
       "func @func_f32(%argA: tensor<10x20x30xf32, #sparseTensor>) -> f32 {\n",
       "  %out_tensor = constant dense<0.0> : tensor<f32>\n",
       "  %reduction = linalg.generic #trait_sum_reduction\n",
       "     ins(%argA: tensor<10x20x30xf32, #sparseTensor>)\n",
       "    outs(%out_tensor: tensor<f32>) {\n",
       "      ^bb(%a: f32, %x: f32):\n",
       "        %0 = addf %x, %a : f32\n",
       "        linalg.yield %0 : f32\n",
       "  } -> tensor<f32>\n",
       "  %answer = tensor.extract %reduction[] : tensor<f32>\n",
       "  return %answer : f32\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-opinion",
   "metadata": {},
   "source": [
    "This large output may seem intimidating due to it's size, but it's mostly just large since it's showing the inputs to each pass. \n",
    "\n",
    "We know that the error happens when the `builtin.unrealized_conversion_cast` operation occurs. \n",
    "\n",
    "We can see from the output above that it happens during the `convert-std-to-llvm` pass. \n",
    "\n",
    "It's likely that there's something problematic in the input to that pass, so it's worth looking into the IR that was given to the `convert-std-to-llvm` pass, which we can see under the section labelled `Input to convert-std-to-llvm`. We'll show a sort snippet of it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b0cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "  Input to convert-std-to-llvm  \n",
      "================================\n",
      "             10        20        30        40        50        60        70        80        90        100       110       120       130       140       \n",
      "    1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345\n",
      "    -------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1|builtin.module  {\n",
      "  2|  llvm.func @memrefCopy(i64, !llvm.ptr<struct<(i64, ptr<i8>)>>, !llvm.ptr<struct<(i64, ptr<i8>)>>)\n",
      "  3|  llvm.func @malloc(i64) -> !llvm.ptr<i8>\n",
      "  4|  llvm.mlir.global private constant @__constant_xf32(0.000000e+00 : f32) : f32\n",
      "  5|  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
      "  6|  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
      "  7|  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
      "  8|    %c2 = constant 2 : index\n",
      "  9|    %c0 = constant 0 : index\n",
      " 10|    %c1 = constant 1 : index\n",
      " 11|    %0 = llvm.mlir.constant(1 : index) : i64\n",
      " 12|    %1 = llvm.mlir.null : !llvm.ptr<f32>\n",
      " 13|    %2 = llvm.getelementptr %1[%0] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>\n",
      " 14|    %3 = llvm.ptrtoint %2 : !llvm.ptr<f32> to i64\n",
      " 15|    %4 = llvm.mlir.addressof @__constant_xf32 : !llvm.ptr<f32>\n"
     ]
    }
   ],
   "source": [
    "result_string = str(result)\n",
    "print(\"\\n\".join(result_string.splitlines()[9:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ace11",
   "metadata": {},
   "source": [
    "While this is a good idea in general, it doesn't seem to be useful here. When MLIR applies a pass, that pass is applied until quiescence, i.e. it keeps applying the pass until nothing changes (or until some limit on the number of applications is reached). \n",
    "\n",
    "It seems that the `convert-std-to-llvm` pass has already been applied a few times since we see several ops from the LLVM dialect already present in the IR shown under the `Input to convert-std-to-llvm` section (for example, we see `llvm.mlir.constant`). \n",
    "\n",
    "Another good place to look is in the output of the last pass right before we get our error. Let's look at the result of the `convert-memref-to-llvm` pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363d28d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "  Input to convert-memref-to-llvm  \n",
      "===================================\n",
      "builtin.module  {\n",
      "  memref.global \"private\" constant @__constant_xf32 : memref<f32> = dense<0.000000e+00>\n",
      "  builtin.func private @sparseValuesF32(!llvm.ptr<i8>) -> memref<?xf32>\n",
      "  builtin.func private @sparsePointers64(!llvm.ptr<i8>, index) -> memref<?xi64>\n",
      "  builtin.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
      "    %c2 = constant 2 : index\n",
      "    %c0 = constant 0 : index\n",
      "    %c1 = constant 1 : index\n",
      "    %0 = memref.get_global @__constant_xf32 : memref<f32>\n",
      "    %1 = call @sparsePointers64(%arg0, %c0) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
      "    %2 = call @sparsePointers64(%arg0, %c1) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
      "    %3 = call @sparsePointers64(%arg0, %c2) : (!llvm.ptr<i8>, index) -> memref<?xi64>\n",
      "    %4 = call @sparseValuesF32(%arg0) : (!llvm.ptr<i8>) -> memref<?xf32>\n",
      "    %5 = memref.alloc() : memref<f32>\n",
      "    memref.copy %0, %5 : memref<f32> to memref<f32>\n",
      "    %6 = memref.load %1[%c0] : memref<?xi64>\n",
      "    %7 = index_cast %6 : i64 to index\n",
      "    %8 = memref.load %1[%c1] : memref<?xi64>\n",
      "    %9 = index_cast %8 : i64 to index\n",
      "    scf.for %arg1 = %7 to %9 step %c1 {\n",
      "      %11 = memref.load %2[%arg1] : memref<?xi64>\n",
      "      %12 = index_cast %11 : i64 to index\n",
      "      %13 = addi %arg1, %c1 : index\n",
      "      %14 = memref.load %2[%13] : memref<?xi64>\n",
      "      %15 = index_cast %14 : i64 to index\n",
      "      scf.for %arg2 = %12 to %15 step %c1 {\n",
      "        %16 = memref.load %3[%arg2] : memref<?xi64>\n",
      "        %17 = index_cast %16 : i64 to index\n",
      "        %18 = addi %arg2, %c1 : index\n",
      "        %19 = memref.load %3[%18] : memref<?xi64>\n",
      "        %20 = index_cast %19 : i64 to index\n",
      "        %21 = memref.load %5[] : memref<f32>\n",
      "        %22 = scf.for %arg3 = %17 to %20 step %c1 iter_args(%arg4 = %21) -> (f32) {\n",
      "          %23 = memref.load %4[%arg3] : memref<?xf32>\n",
      "          %24 = addf %arg4, %23 : f32\n",
      "          scf.yield %24 : f32\n",
      "        }\n",
      "        memref.store %22, %5[] : memref<f32>\n",
      "      }\n",
      "    }\n",
      "    %10 = memref.load %5[] : memref<f32>\n",
      "    return %10 : f32\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(result_string.splitlines()[139:187]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee2493",
   "metadata": {},
   "source": [
    "We see that the ops are mostly ops from te standard dialect. However, there are some ops from the `scf` dialect. It's unclear whether or not the `convert-std-to-llvm` dialect can handle ops from the `scf` dialect. Given the name of the `convert-std-to-llvm` pass, we can infer that it can only handle ops from the `std` dialect and cannot handle ops from the `scf` dialect. Let's see if there are any passes that can convert from the `scg` dialect to the `std` dialect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de378332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Dialects: acc, affine, amx, arm_neon, arm_sve, async, builtin, complex, dlti, emitc, gpu, linalg, llvm, math, memref, nvvm, omp, pdl, pdl_interp, quant, rocdl, scf, shape, sparse_tensor, spv, std, tensor, test, tosa, vector, x86vector\r\n",
      "      --async-parallel-for                              -   Convert scf.parallel operations to multiple async compute ops executed concurrently for non-overlapping iteration ranges\r\n",
      "      --convert-linalg-tiled-loops-to-scf               -   Lower linalg tiled loops to SCF loops and parallel loops\r\n",
      "      --convert-openacc-to-scf                          -   Convert the OpenACC ops to OpenACC with SCF dialect\r\n",
      "      --convert-parallel-loops-to-gpu                   -   Convert mapped scf.parallel ops to gpu launch operations\r\n",
      "      --convert-scf-to-openmp                           -   Convert SCF parallel loop to OpenMP parallel + workshare constructs.\r\n",
      "      --convert-scf-to-spirv                            -   Convert SCF dialect to SPIR-V dialect.\r\n",
      "      --convert-scf-to-std                              -   Convert SCF dialect to Standard dialect, replacing structured control flow with a CFG\r\n",
      "      --convert-vector-to-scf                           -   Lower the operations from the vector dialect into the SCF dialect\r\n",
      "      --scf-bufferize                                   -   Bufferize the scf dialect.\r\n",
      "        --test-affine-min-scf-canonicalization-patterns - Test affine-min + scf canonicalization patterns.\r\n",
      "      --test-scf-for-utils                              -   test scf.for utils\r\n",
      "      --test-scf-if-utils                               -   test scf.if utils\r\n",
      "      --test-scf-pipelining                             -   test scf.forOp pipelining\r\n",
      "      --test-vector-transfer-full-partial-split         -   Test conversion patterns to split transfer ops via scf.if + linalg ops\r\n",
      "      --tosa-to-scf                                     -   Lower TOSA to the SCF dialect\r\n"
     ]
    }
   ],
   "source": [
    "!mlir-opt --help | grep \"scf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-round",
   "metadata": {},
   "source": [
    "The pass `convert-scf-to-std` seems promising as it intends to convert the `scf` dialect to `std` dialect. \n",
    "\n",
    "Let's see if running the `convert-scf-to-std` pass right before the `convert-std-to-llvm` pass will get rid of our exception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ideal-disposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builtin.module attributes {llvm.data_layout = \"\"}  {\n",
      "  llvm.func @memrefCopy(i64, !llvm.ptr<struct<(i64, ptr<i8>)>>, !llvm.ptr<struct<(i64, ptr<i8>)>>)\n",
      "  llvm.func @malloc(i64) -> !llvm.ptr<i8>\n",
      "  llvm.mlir.global private constant @__constant_xf32(0.000000e+00 : f32) : f32\n",
      "  llvm.func @sparseValuesF32(!llvm.ptr<i8>) -> !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> attributes {sym_visibility = \"private\"}\n",
      "  llvm.func @sparsePointers64(!llvm.ptr<i8>, i64) -> !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> attributes {sym_visibility = \"private\"}\n",
      "  llvm.func @func_f32(%arg0: !llvm.ptr<i8>) -> f32 {\n",
      "    %0 = llvm.mlir.constant(2 : index) : i64\n",
      "    %1 = llvm.mlir.constant(0 : index) : i64\n",
      "    %2 = llvm.mlir.constant(1 : index) : i64\n",
      "    %3 = llvm.mlir.constant(1 : index) : i64\n",
      "    %4 = llvm.mlir.null : !llvm.ptr<f32>\n",
      "    %5 = llvm.getelementptr %4[%3] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>\n",
      "    %6 = llvm.ptrtoint %5 : !llvm.ptr<f32> to i64\n",
      "    %7 = llvm.mlir.addressof @__constant_xf32 : !llvm.ptr<f32>\n",
      "    %8 = llvm.mlir.constant(0 : index) : i64\n",
      "    %9 = llvm.getelementptr %7[%8] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>\n",
      "    %10 = llvm.mlir.constant(3735928559 : index) : i64\n",
      "    %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>\n",
      "    %12 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
      "    %13 = llvm.insertvalue %11, %12[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64)>\n",
      "    %14 = llvm.insertvalue %9, %13[1] : !llvm.struct<(ptr<f32\n"
     ]
    }
   ],
   "source": [
    "passes = [\n",
    "    \"--sparsification\",\n",
    "    \"--sparse-tensor-conversion\",\n",
    "    \"--linalg-bufferize\",\n",
    "    \"--func-bufferize\",\n",
    "    \"--tensor-bufferize\",\n",
    "    \"--tensor-constant-bufferize\",\n",
    "    \"--finalizing-bufferize\",\n",
    "    \"--convert-linalg-to-loops\",\n",
    "    \"--convert-memref-to-llvm\",\n",
    "    \"--convert-scf-to-std\", # newly added\n",
    "    \"--convert-std-to-llvm\",\n",
    "]\n",
    "result = cli.apply_passes(mlir_bytes, passes)\n",
    "print(result[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-parks",
   "metadata": {},
   "source": [
    "It looks like it fixed our issue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
