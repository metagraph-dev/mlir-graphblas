{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greenhouse-advocacy",
   "metadata": {},
   "source": [
    "# JIT Engine: Tensor Summation\n",
    "\n",
    "This example will go over how to compile MLIR code to a function callable from Python.\n",
    "\n",
    "The example MLIR code we’ll use here performs tensor summation.\n",
    "\n",
    "Let’s first import some necessary modules and generate an instance of our JIT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "selected-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlir_graphblas\n",
    "import numpy as np\n",
    "\n",
    "engine = mlir_graphblas.MlirJitEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-fields",
   "metadata": {},
   "source": [
    "We'll use the same set of passes to optimize and compile all of our examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "executed-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = [\n",
    "    \"--graphblas-structuralize\",\n",
    "    \"--graphblas-optimize\",\n",
    "    \"--graphblas-lower\",\n",
    "    \"--sparsification\",\n",
    "    \"--sparse-tensor-conversion\",\n",
    "    \"--linalg-bufferize\",\n",
    "    \"--func-bufferize\",\n",
    "    \"--tensor-constant-bufferize\",\n",
    "    \"--tensor-bufferize\",\n",
    "    \"--finalizing-bufferize\",\n",
    "    \"--convert-linalg-to-loops\",\n",
    "    \"--convert-scf-to-std\",\n",
    "    \"--convert-memref-to-llvm\",\n",
    "    \"--convert-math-to-llvm\",\n",
    "    \"--convert-openmp-to-llvm\",\n",
    "    \"--convert-arith-to-llvm\",\n",
    "    \"--convert-math-to-llvm\",\n",
    "    \"--convert-std-to-llvm\",\n",
    "    \"--reconcile-unrealized-casts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-firewall",
   "metadata": {},
   "source": [
    "We'll use this MLIR code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_text = \"\"\"\n",
    "#trait_sum_reduction = {\n",
    "  indexing_maps = [\n",
    "    affine_map<(i,j,k) -> (i,j,k)>,\n",
    "    affine_map<(i,j,k) -> ()>\n",
    "  ],\n",
    "  iterator_types = [\"reduction\", \"reduction\", \"reduction\"]\n",
    "}\n",
    "\n",
    "func @tensor_sum(%argA: tensor<2x3x5xf32>) -> f32 {\n",
    "  %output_storage = arith.constant dense<0.0> : tensor<f32>\n",
    "  %reduction = linalg.generic #trait_sum_reduction\n",
    "    ins(%argA: tensor<2x3x5xf32>)\n",
    "    outs(%output_storage: tensor<f32>) {\n",
    "      ^bb(%a: f32, %x: f32):\n",
    "        %0 = arith.addf %x, %a : f32\n",
    "        linalg.yield %0 : f32\n",
    "  } -> tensor<f32>\n",
    "  %answer = tensor.extract %reduction[] : tensor<f32>\n",
    "  return %answer : f32\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-pharmaceutical",
   "metadata": {},
   "source": [
    "Let's compile our MLIR code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "superior-layer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tensor_sum']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.add(mlir_text, passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-franchise",
   "metadata": {},
   "source": [
    "Let's try out our compiled function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pretty-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab our callable\n",
    "tensor_sum = engine.tensor_sum\n",
    "\n",
    "# generate inputs\n",
    "a = np.arange(30, dtype=np.float32).reshape([2,3,5])\n",
    "\n",
    "# generate output\n",
    "result = tensor_sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorporated-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-funds",
   "metadata": {},
   "source": [
    "Let's verify that our function works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "golden-leeds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result == a.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-terror",
   "metadata": {},
   "source": [
    "The examples up to this point make it so that we now know how to perform element-wise operations and reduction operations (e.g. summation). \n",
    "\n",
    "With this knowledge, it's fairly straightforward to implement matrix multiplication and dot product calculation. We'll leave this as an exercise for the reader. If help is needed, it's useful to know that matrix multiplication and dot product calculation have already been directly implemented in the `linalg` dialect:\n",
    "\n",
    "- [linalg.matmul](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgmatmul-mlirlinalgmatmulop)\n",
    "- [linalg.matmul_i8_i8_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgmatmul_i8_i8_i32-mlirlinalgmatmuli8i8i32op)\n",
    "- [linalg.matmul_i32_i32_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgmatmul_i32_i32_i32-mlirlinalgmatmuli32i32i32op)\n",
    "- [linalg.matmul_i16_i16_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgmatmul_i16_i16_i32-mlirlinalgmatmuli16i16i32op)\n",
    "- [linalg.matmul_column_major](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgmatmul_column_major-mlirlinalgmatmulcolumnmajoropx)\n",
    "- [linalg.batch_matmul](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgbatch_matmul-mlirlinalgbatchmatmulop)\n",
    "- [linalg.batch_matmul_i8_i8_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgbatch_matmul_i8_i8_i32-mlirlinalgbatchmatmuli8i8i32op)\n",
    "- [linalg.batch_matmul_i32_i32_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgbatch_matmul_i32_i32_i32-mlirlinalgbatchmatmuli32i32i32op)\n",
    "- [linalg.batch_matmul_i16_i16_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgbatch_matmul_i16_i16_i32-mlirlinalgbatchmatmuli16i16i32op)\n",
    "- [linalg.dot](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgdot-mlirlinalgdotop)\n",
    "- [linalg.dot_i8_i8_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgdot_i8_i8_i32-mlirlinalgdoti8i8i32op)\n",
    "- [linalg.dot_i32_i32_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgdot_i32_i32_i32-mlirlinalgdoti32i32i32op)\n",
    "- [linalg.dot_i16_i16_i32](https://mlir.llvm.org/docs/Dialects/Linalg/#linalgdot_i16_i16_i32-mlirlinalgdoti16i16i32op)\n",
    "\n",
    "One useful skill worth practicing is seeing how these `linalg` operations lower to see what's going on and comparing those to how we might've implemented things. The MLIR explorer can come in handy for this purpose. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
