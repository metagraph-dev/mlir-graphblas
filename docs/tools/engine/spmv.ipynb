{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legitimate-celebration",
   "metadata": {},
   "source": [
    "# JIT Engine: Sparse Matrix x Dense Vector\n",
    "\n",
    "Most of the previous tutorials have been focused on dense tensors. This tutorial will focus on sparse tensors. \n",
    "\n",
    "In particular, this example will go over how to compile MLIR code aimed at multiplying a sparse matrix with a dense tensor into a function callable from Python. \n",
    "\n",
    "Let’s first import some necessary modules and generate an instance of our JIT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coupled-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlir_graphblas\n",
    "import mlir_graphblas.sparse_utils\n",
    "import numpy as np\n",
    "\n",
    "engine = mlir_graphblas.MlirJitEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-deposit",
   "metadata": {},
   "source": [
    "## State of MLIR's Current Sparse Tensor Support\n",
    "\n",
    "MLIR's sparse tensor support is in its early stages and is fairly limited as it is undergoing frequent development. For more details on what is currently being focused on, see [the MLIR discussion on sparse tensors](https://llvm.discourse.group/t/mlir-support-for-sparse-tensors/2020).\n",
    "\n",
    "It currently has two noteworthy limitations:\n",
    "\n",
    "- MLIR's sparse tensor functionality in the `linalg` dialect currently only supports reading from sparse tensors but not storing into sparse tensors. Thus, the functions we write can accept sparse tensors as inputs but will return dense tensors.\n",
    "- MLIR's sparse tensor support only supports a limited number of [sparse storage layouts](https://en.wikipedia.org/wiki/Sparse_matrix#Storing_a_sparse_matrix). \n",
    "\n",
    "This first tutorial will go over the details of MLIR's sparse tensor support along with how to implement a function to multiply an MLIR sparse matrix with a dense vector to create a dense matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-bristol",
   "metadata": {},
   "source": [
    "## MLIR’s Sparse Tensor Data Structure Overview\n",
    "\n",
    "MLIR's sparse tensors are implemented as structs with several array and vector attributes used to store the tensor's elements. The source code for the struct representing MLIR's sparse tensor can be found [here](https://github.com/llvm/llvm-project/blob/main/mlir/lib/ExecutionEngine/SparseUtils.cpp).\n",
    "\n",
    "The JIT engine provides `mlir_graphblas.sparse_utils.MLIRSparseTensor`, a wrapper around MLIR's sparse tensor struct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decent-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sparse tensor below looks like this (where the underscores represent zeros):\n",
    "# \n",
    "# [[1.2, ___, ___, ___, ___, ___, ___, ___, ___, ___],\n",
    "#  [___, ___, ___, 3.4, ___, ___, ___, ___, ___, ___],\n",
    "#  [___, ___, 5.6, ___, ___, ___, ___, ___, ___, ___],\n",
    "#  [___, ___, ___, ___, ___, ___, ___, ___, ___, ___],\n",
    "#  [___, ___, ___, ___, ___, ___, ___, 7.8, ___, ___],\n",
    "#  [___, ___, ___, ___, ___, ___, ___, ___, ___, ___],\n",
    "#  [___, ___, ___, ___, ___, ___, ___, ___, ___, ___],\n",
    "#  [___, ___, ___, ___, ___, ___, ___, ___, ___, ___],\n",
    "#  [___, ___, ___, ___, ___, ___, ___, ___, ___, ___],\n",
    "#  [___, ___, ___, ___, ___, ___, ___, ___, ___, 9.0]]\n",
    "# \n",
    "\n",
    "indices = np.array([\n",
    "    [0, 0], \n",
    "    [1, 3], \n",
    "    [2, 2], \n",
    "    [4, 7], \n",
    "    [9, 9],\n",
    "], dtype=np.uint64) # Coordinates\n",
    "values = np.array([1.2, 3.4, 5.6, 7.8, 9.0], dtype=np.float32) # values at each coordinate\n",
    "sizes = np.array([10, 10], dtype=np.uint64) # tensor shape\n",
    "sparsity = np.array([True, True], dtype=np.bool8) # a boolean for each dimension telling which dimensions are sparse\n",
    "\n",
    "sparse_tensor = mlir_graphblas.sparse_utils.MLIRSparseTensor(indices, values, sizes, sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-steps",
   "metadata": {},
   "source": [
    "To initialize an instance of `mlir_graphblas.sparse_utils.MLIRSparseTensor`, we need to provide:\n",
    "\n",
    "- The coordinates of each non-zero position in the sparse tensor (see the variable `indices` above).\n",
    "- The values at each position (see the variable `values` above). There's a one-to-one correspondence between each coordinate and each value (order matters here).\n",
    "- The shape of the sparse tensor (see the variable `sizes` above).\n",
    "- The sparsity of each dimension (see the variable `sparsity` above). This determines the sparsity/data layout, e.g. a matrix dense in the 0th dimension and sparse in the second dimension has a [CSR](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_%28CSR,_CRS_or_Yale_format%29) data layout. For more information on how the sparse data layouts work, see [the MLIR discussion on sparse tensors](https://llvm.discourse.group/t/mlir-support-for-sparse-tensors/2020). \n",
    "\n",
    "Despite the fact that we give the positions and values of the non-zero elements to the constructor in a way that resembles [COO format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_%28COO%29), the underlying data structure does not store them in [COO format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_%28COO%29). The sparsity of each dimension (see the variable `sparsity` above) is what the constructor uses to determine how to store the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-punishment",
   "metadata": {},
   "source": [
    "## Using MLIR’s Sparse Tensor Data Structure in MLIR Code\n",
    "\n",
    "We'll now go over how we can use the MLIR's sparse tensor in some MLIR code. \n",
    "\n",
    "Here's the MLIR code for [multiplying a sparse matrix with a dense tensor](https://en.wikipedia.org/wiki/Sparse_matrix-vector_multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sporting-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_text = \"\"\"\n",
    "#trait_matvec = {\n",
    "  indexing_maps = [\n",
    "    affine_map<(i,j) -> (i,j)>,\n",
    "    affine_map<(i,j) -> (j)>,\n",
    "    affine_map<(i,j) -> (i)>\n",
    "  ],\n",
    "  iterator_types = [\"parallel\", \"reduction\"],\n",
    "  sparse = [\n",
    "    [ \"S\", \"S\" ], \n",
    "    [ \"D\" ],\n",
    "    [ \"D\" ]\n",
    "  ],\n",
    "  sparse_dim_map = [\n",
    "    affine_map<(i,j) -> (j,i)>,\n",
    "    affine_map<(i)   -> (i)>,\n",
    "    affine_map<(i)   -> (i)>\n",
    "  ]\n",
    "}\n",
    "\n",
    "#HyperSparseMatrix = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"compressed\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j) -> (i,j)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "func @spmv(%arga: tensor<10x10xf32, #HyperSparseMatrix>, %argb: tensor<10xf32>) -> tensor<10xf32> {\n",
    "  %output_storage = constant dense<0.0> : tensor<10xf32>\n",
    "  %0 = linalg.generic #trait_matvec\n",
    "    ins(%arga, %argb : tensor<10x10xf32, #HyperSparseMatrix>, tensor<10xf32>)\n",
    "    outs(%output_storage: tensor<10xf32>) {\n",
    "      ^bb(%A: f32, %b: f32, %x: f32):\n",
    "        %0 = mulf %A, %b : f32\n",
    "        %1 = addf %x, %0 : f32\n",
    "        linalg.yield %1 : f32\n",
    "    } -> tensor<10xf32>\n",
    "  return %0 : tensor<10xf32>\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-carnival",
   "metadata": {},
   "source": [
    "One thing to note about the trait `#trait_matvec` used here that makes it different from the traits used by our dense operations we've shown in previous tutorials is that it specifies the sparsity via the `sparse` attribute. Note the presence of `[ \"S\", \"S\" ]`. This must correspond to the sparsity of our sparse tensor (see the Python variable `sparsity` from earlier). \n",
    "\n",
    "Also, note the type of our sparse tensor. The type is `!SparseTensor`, which is an MLIR alias for the type `!llvm.ptr<i8>` from the [LLVM dialect](https://mlir.llvm.org/docs/Dialects/LLVM/). MLIR's passes for sparse tensors are currently under development and treat pointers to 8-bit integers as pointers to a sparse tensor struct. MLIR's sparse tensor passes are able to differentiate normal uses of pointers to 8-bit integers from pointers to a sparse tensor struct via the use of the `linalg.sparse_tensor` operation. Only the results of `linalg.sparse_tensor` are treated as sparse tensors. This is a likely a temporary measure implemented as a prototype that is expected to change into a more mature piece of functionality in the upcoming months.\n",
    "\n",
    "The results from `linalg.sparse_tensor` operations can be treated as normal tensors with all the complexities of indexing into the sparse tensor handled by MLIR's sparse tensor passes. \n",
    "\n",
    "The MLIR sparse tensor pass that we'll use to lower our sparse tensors is `--test-sparsification=lower`. Here are all the passes we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "focal-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = [\n",
    "    \"--sparsification\",\n",
    "    \"--sparse-tensor-conversion\",\n",
    "    \"--linalg-bufferize\",\n",
    "    \"--func-bufferize\",\n",
    "    \"--tensor-bufferize\",\n",
    "    \"--tensor-constant-bufferize\",\n",
    "    \"--finalizing-bufferize\",\n",
    "    \"--convert-linalg-to-loops\",\n",
    "    \"--convert-scf-to-std\",\n",
    "    \"--convert-memref-to-llvm\",\n",
    "    \"--convert-std-to-llvm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-artwork",
   "metadata": {},
   "source": [
    "## SpMV Compilation\n",
    "\n",
    "Let's now actually see what our MLIR code can do. \n",
    "\n",
    "We'll first compile our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "parliamentary-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.add(mlir_text, passes)\n",
    "spmv = engine.spmv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-bridal",
   "metadata": {},
   "source": [
    "We already have a 10x10 sparse tensor from earlier (see the Python variable `sparse_tensor`) that we can use as an input. Let's create a dense vector we can multiply it by. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coated-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_vector = np.arange(10, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-browse",
   "metadata": {},
   "source": [
    "Let's perform the calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acknowledged-dutch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.      , 10.200001, 11.2     ,  0.      , 54.600002,  0.      ,\n",
       "        0.      ,  0.      ,  0.      , 81.      ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spmv_answer = spmv(sparse_tensor, dense_vector)\n",
    "spmv_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-century",
   "metadata": {},
   "source": [
    "Let's verify if this is the result we expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "falling-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_tensor = np.array([\n",
    " [1.2, 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  ],\n",
    " [0  , 0  , 0  , 3.4, 0  , 0  , 0  , 0  , 0  , 0  ],\n",
    " [0  , 0  , 5.6, 0  , 0  , 0  , 0  , 0  , 0  , 0  ],\n",
    " [0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  ],\n",
    " [0  , 0  , 0  , 0  , 0  , 0  , 0  , 7.8, 0  , 0  ],\n",
    " [0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  ],\n",
    " [0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  ],\n",
    " [0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  ],\n",
    " [0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  ],\n",
    " [0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 0  , 9.0]\n",
    "], dtype=np.float32)\n",
    "np_answer = dense_tensor @ dense_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "charitable-vertex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(spmv_answer == np_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
