{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df94aea",
   "metadata": {},
   "source": [
    "# Matrix Ops\n",
    "\n",
    "This example will go over how to use the `--graphblas-lower` pass from `graphblas-opt` to lower several ops from the GraphBLAS dialect that deal with [CSR](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)) or CSC matrices.  This tutorial does not exhaustively cover all such ops. It merely intends to provide some examples to learn from. \n",
    "\n",
    "Let’s first import some necessary modules and generate an instance of our JIT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9a93b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlir_graphblas\n",
    "import numpy as np\n",
    "from mlir_graphblas.tools.utils import sparsify_array\n",
    "\n",
    "engine = mlir_graphblas.MlirJitEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251bd4a",
   "metadata": {},
   "source": [
    "Here are the passes we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5430c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = [\n",
    "    \"--graphblas-structuralize\",\n",
    "    \"--graphblas-optimize\",\n",
    "    \"--graphblas-lower\",\n",
    "    \"--sparsification\",\n",
    "    \"--sparse-tensor-conversion\",\n",
    "    \"--linalg-bufferize\",\n",
    "    \"--func-bufferize\",\n",
    "    \"--tensor-constant-bufferize\",\n",
    "    \"--tensor-bufferize\",\n",
    "    \"--finalizing-bufferize\",\n",
    "    \"--convert-linalg-to-loops\",\n",
    "    \"--convert-scf-to-std\",\n",
    "    \"--convert-memref-to-llvm\",\n",
    "    \"--convert-math-to-llvm\",\n",
    "    \"--convert-openmp-to-llvm\",\n",
    "    \"--convert-arith-to-llvm\",\n",
    "    \"--convert-math-to-llvm\",\n",
    "    \"--convert-std-to-llvm\",\n",
    "    \"--reconcile-unrealized-casts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71ba14",
   "metadata": {},
   "source": [
    "## Overview of graphblas.matrix_multiply\n",
    "\n",
    "Here, we'll show how to use the `graphblas.matrix_multiply` op. \n",
    "\n",
    "`graphblas.matrix_multiply` performs a matrix multiply according to the given semiring and optional structural mask. See the [ops reference](../../ops_reference.rst) for further details regarding the intended behavior.\n",
    "\n",
    "We'll show some examples here of how to use `graphblas.matrix_multiply`.\n",
    "\n",
    "Let's first create some example input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d263eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dense = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0],\n",
    "        [2, 0, 3, 4],\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 5, 6],\n",
    "    ],\n",
    "    dtype=np.float64\n",
    ")\n",
    "A = sparsify_array(A_dense, [False, True])\n",
    "\n",
    "B_dense = np.array(\n",
    "    [\n",
    "        [0, 7, 0, 7],\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 1, 7, 0],\n",
    "        [0, 7, 2, 0],\n",
    "    ],\n",
    "    dtype=np.float64\n",
    ")\n",
    "B = sparsify_array(B_dense, [False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cf346",
   "metadata": {},
   "source": [
    "Here is some MLIR code to perform a conventional matrix-multiply by using `graphblas.matrix_multiply` with the plus-times semiring.\n",
    "\n",
    "Note that both of the matrices above are CSR matrices. Thus, we'll need to convert the layout of the second operand in our code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cac3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_text = \"\"\"\n",
    "#CSR64 = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"dense\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j) -> (i,j)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "#CSC64 = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"dense\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j) -> (j,i)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "module {\n",
    "    func @matrix_multiply_plus_times(%a: tensor<?x?xf64, #CSR64>, %b: tensor<?x?xf64, #CSR64>) -> tensor<?x?xf64, #CSR64> {\n",
    "        %b_csc = graphblas.convert_layout %b : tensor<?x?xf64, #CSR64> to tensor<?x?xf64, #CSC64>\n",
    "        %answer = graphblas.matrix_multiply %a, %b_csc { semiring = \"plus_times\" } : (tensor<?x?xf64, #CSR64>, tensor<?x?xf64, #CSC64>) to tensor<?x?xf64, #CSR64>\n",
    "        return %answer : tensor<?x?xf64, #CSR64>\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f5486",
   "metadata": {},
   "source": [
    "Let's run it and verify that it gets the same results as [NumPy](https://numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1ec541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matrix_multiply_plus_times']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.add(mlir_text, passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894e1b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matmul_result = engine.matrix_multiply_plus_times(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdba414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  7.,  0.,  7.],\n",
       "       [ 0., 45., 29., 14.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0., 47., 47.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matmul_result.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9516b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(A_dense @ B_dense == sparse_matmul_result.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48677164",
   "metadata": {},
   "source": [
    "We'll now show how to perform a matrix-multiply with the plus-plus semiring with and without a structural mask to show how the behavior differs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b4eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sparsify_array(\n",
    "    np.array(\n",
    "        [\n",
    "            [0, 0, 9, 8],\n",
    "            [0, 0, 7, 6],\n",
    "            [0, 0, 5, 4],\n",
    "            [0, 0, 3, 2],\n",
    "        ],\n",
    "        dtype=np.float64\n",
    "    ),\n",
    "    [False, True]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21383c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_text = \"\"\"\n",
    "#CSR64 = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"dense\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j) -> (i,j)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "#CSC64 = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"dense\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j) -> (j,i)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "module {\n",
    "    func @matrix_multiply_plus_plus_no_mask(%a: tensor<?x?xf64, #CSR64>, %b: tensor<?x?xf64, #CSR64>) -> tensor<?x?xf64, #CSR64> {\n",
    "        %b_csc = graphblas.convert_layout %b : tensor<?x?xf64, #CSR64> to tensor<?x?xf64, #CSC64>\n",
    "        %answer = graphblas.matrix_multiply %a, %b_csc { semiring = \"plus_plus\" } : (tensor<?x?xf64, #CSR64>, tensor<?x?xf64, #CSC64>) to tensor<?x?xf64, #CSR64>\n",
    "        return %answer : tensor<?x?xf64, #CSR64>\n",
    "    }\n",
    "    \n",
    "    func @matrix_multiply_plus_plus(%a: tensor<?x?xf64, #CSR64>, %b: tensor<?x?xf64, #CSR64>, %m: tensor<?x?xf64, #CSR64>) -> tensor<?x?xf64, #CSR64> {\n",
    "        %b_csc = graphblas.convert_layout %b : tensor<?x?xf64, #CSR64> to tensor<?x?xf64, #CSC64>\n",
    "        %answer = graphblas.matrix_multiply %a, %b_csc, %m { semiring = \"plus_plus\" } : (tensor<?x?xf64, #CSR64>, tensor<?x?xf64, #CSC64>, tensor<?x?xf64, #CSR64>) to tensor<?x?xf64, #CSR64>\n",
    "        return %answer : tensor<?x?xf64, #CSR64>\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ba6bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matrix_multiply_plus_plus_no_mask', 'matrix_multiply_plus_plus']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.add(mlir_text, passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_mask_result = engine.matrix_multiply_plus_plus_no_mask(A, B)\n",
    "with_mask_result = engine.matrix_multiply_plus_plus(A, B, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9b0dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  8.,  0.,  8.],\n",
       "       [ 0., 24., 16.,  9.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0., 19., 20.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_mask_result.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69451c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  8.],\n",
       "       [ 0.,  0., 16.,  9.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., 20.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_mask_result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83638bad",
   "metadata": {},
   "source": [
    "Note how the results in the masked output only have elements present in the positions where the mask had elements present. \n",
    "\n",
    "Since we can't verify the results via NumPy given that it doesn't support semirings in its matrix multiply implementation, we'll leave the task of verifying the results as an exercise for the reader. Note that if we're applying the element-wise operation to the values at two positions (one each sparse tensor) and one position has a value but not the other does not, then the element-wise operation for these two positions will contribute no value to be aggregated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb91b9d0",
   "metadata": {},
   "source": [
    "Next, we'll show how the `mask_complement` attribute for `graphblas.matrix_multiply` works. It simply makes the `graphblas.matrix_multiply` only calculate output values in positions not present in the mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0914b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_text = \"\"\"\n",
    "#CSR64 = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"dense\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j) -> (i,j)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "#CSC64 = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"dense\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j) -> (j,i)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "module {\n",
    "    func @matrix_multiply_plus_plus_mask_complement(%a: tensor<?x?xf64, #CSR64>, %b: tensor<?x?xf64, #CSR64>, %m: tensor<?x?xf64, #CSR64>) -> tensor<?x?xf64, #CSR64> {\n",
    "        %b_csc = graphblas.convert_layout %b : tensor<?x?xf64, #CSR64> to tensor<?x?xf64, #CSC64>\n",
    "        %answer = graphblas.matrix_multiply %a, %b_csc, %m { semiring = \"plus_plus\", mask_complement = true } : (tensor<?x?xf64, #CSR64>, tensor<?x?xf64, #CSC64>, tensor<?x?xf64, #CSR64>) to tensor<?x?xf64, #CSR64>\n",
    "        return %answer : tensor<?x?xf64, #CSR64>\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c6b817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matrix_multiply_plus_plus_mask_complement']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.add(mlir_text, passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86963cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_complement_result = engine.matrix_multiply_plus_plus_mask_complement(A, B, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fdc2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  8.,  0.,  0.],\n",
       "       [ 0., 24.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0., 19.,  0.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_complement_result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644ff97",
   "metadata": {},
   "source": [
    "All of the above behavior works similarly with vectors. We'll leave exploring that as an exercise for the reader. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3792a2",
   "metadata": {},
   "source": [
    "## Overview of graphblas.apply\n",
    "\n",
    "Here, we'll show how to use the `graphblas.apply` op.\n",
    "\n",
    "`graphblas.apply` applies in an element-wise fashion the function indicated by the `apply_operator` attribute to each element of the given sparse tensor. The operator can be unary or binary. Binary operators require a thunk. The supported binary operators are “min”, “div”, and “fill”. Unary operators cannot take a thunk. Unary operators cannot take a thunk. The supported unary operators are “abs”, “minv” (i.e. multiplicative inverse or 1/x), “ainv” (i.e. additive inverse or -x), and “identity”.\n",
    "\n",
    "The given sparse tensor must either be a CSR matrix, CSC matrix, or a sparse vector.\n",
    "\n",
    "We'll show some examples here of how to use `graphblas.apply`.\n",
    "\n",
    "First, we'll clip the values of a sparse matrix to be no higher than a given limit using the \"min\" operator using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "276c6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_text = \"\"\"\n",
    "#CSR64 = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"dense\", \"compressed\" ],\n",
    "  dimOrdering = affine_map<(i,j) -> (i,j)>,\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "module {\n",
    "    func @clip(%sparse_tensor: tensor<?x?xf64, #CSR64>, %limit: f64) -> tensor<?x?xf64, #CSR64> {\n",
    "        %answer = graphblas.apply %sparse_tensor, %limit { apply_operator = \"min\" } : (tensor<?x?xf64, #CSR64>, f64) to tensor<?x?xf64, #CSR64>\n",
    "        return %answer : tensor<?x?xf64, #CSR64>\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a896e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clip']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.add(mlir_text, passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc98b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_result = engine.clip(A, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e91bf86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [2., 0., 3., 3.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 3., 3.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de26747",
   "metadata": {},
   "source": [
    "Using the \"min\" operator is simple since it is a symmetric function. There are some binary operators that are not symmetric, e.g. the \"div\" operator is not symmetric since dividing all the elements in a sparse tensor by `x` will lead to a different result than dividing `x` by each element in a sparse tensor. \n",
    "\n",
    "We'll show some code below that'll demonstrate how to specify the order in which the elements of the given sparse tensor and the thunk are passed to the operator. We'll show this using the \"div\" operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24888228",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_text = \"\"\"\n",
    "#CV64 = #sparse_tensor.encoding<{\n",
    "  dimLevelType = [ \"compressed\" ],\n",
    "  pointerBitWidth = 64,\n",
    "  indexBitWidth = 64\n",
    "}>\n",
    "\n",
    "module {   \n",
    "    func @apply_vector_left_thunk_div(%sparse_tensor: tensor<?xf64, #CV64>, %thunk: f64) -> tensor<?xf64, #CV64> {\n",
    "        %answer = graphblas.apply %thunk, %sparse_tensor { apply_operator = \"div\" } : (f64, tensor<?xf64, #CV64>) to tensor<?xf64, #CV64>\n",
    "        return %answer : tensor<?xf64, #CV64>\n",
    "    }\n",
    "   \n",
    "    func @apply_vector_right_thunk_div(%sparse_tensor: tensor<?xf64, #CV64>, %thunk: f64) -> tensor<?xf64, #CV64> {\n",
    "        %answer = graphblas.apply %sparse_tensor, %thunk { apply_operator = \"div\" } : (tensor<?xf64, #CV64>, f64) to tensor<?xf64, #CV64>\n",
    "        return %answer : tensor<?xf64, #CV64>\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57f8c8",
   "metadata": {},
   "source": [
    "Note how in `apply_vector_left_thunk_div`, the thunk is passed as the first argument to `graphblas.apply`. This means that the thunk is the dividend,\n",
    "\n",
    "Since the thunk is passed as the second argument to `graphblas.apply` in `apply_vector_right_thunk_div`, this means the thunk is the divisor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dc3279e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apply_vector_left_thunk_div', 'apply_vector_right_thunk_div']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.add(mlir_text, passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bd113",
   "metadata": {},
   "source": [
    "Since our code operators on vectors, let's create some vector inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bdbc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_vector = np.array([0, 0, -100, 0, 0, 0, 200, -300, 0, 0, 400, 0, 0], dtype=np.float64)\n",
    "sparse_vector = sparsify_array(dense_vector, [True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02607142",
   "metadata": {},
   "source": [
    "Let's see what we get as outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd20b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thunk_dividend_result = engine.apply_vector_left_thunk_div(sparse_vector, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13355b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        , -0.1       ,  0.        ,  0.        ,\n",
       "        0.        ,  0.05      , -0.03333333,  0.        ,  0.        ,\n",
       "        0.025     ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunk_dividend_result.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2215050",
   "metadata": {},
   "outputs": [],
   "source": [
    "thunk_divisor_result = engine.apply_vector_right_thunk_div(sparse_vector, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8b0e0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0., -10.,   0.,   0.,   0.,  20., -30.,   0.,   0.,  40.,\n",
       "         0.,   0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunk_divisor_result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f404e",
   "metadata": {},
   "source": [
    "Notice that in the result of `thunk_dividend_result.toarray()`, we have zeros where there were missing values. \n",
    "\n",
    "This is somewhat unintuitive. We'd expect some sort of [NaN](https://en.wikipedia.org/wiki/NaN) values where we see zeros since that's the expected result for `10.0/0.0`. This is because `graphblas.apply` only operates on values present in the given sparse tensor and does nothing to the missing values. `MLIRSparseTensor.toarray` treats the missing values as zeros. \n",
    "\n",
    "Our final example for `graphblas.apply` will demonstrate how to apply the absolute value function to each element of a sparse vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1412b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlir_text = \"\"\"\n",
    "#CV64 = #sparse_tensor.encoding<{ \n",
    "    dimLevelType = [ \"compressed\" ], \n",
    "    pointerBitWidth = 64, \n",
    "    indexBitWidth = 64 \n",
    "}>\n",
    "\n",
    "module {\n",
    "    func @vector_abs(%sparse_tensor: tensor<?xf64, #CV64>) -> tensor<?xf64, #CV64> {\n",
    "        %answer = graphblas.apply %sparse_tensor { apply_operator = \"abs\" } : (tensor<?xf64, #CV64>) to tensor<?xf64, #CV64>\n",
    "        return %answer : tensor<?xf64, #CV64>\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "517818ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vector_abs']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.add(mlir_text, passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "130298da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_result = engine.vector_abs(sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b476da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0., 100.,   0.,   0.,   0., 200., 300.,   0.,   0., 400.,\n",
       "         0.,   0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_result.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "094532a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(sparse_result.toarray() == np.abs(dense_vector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
